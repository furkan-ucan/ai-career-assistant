"""
Embedding Servisi
Google Gemini API kullanarak metin embeddings'leri oluÅŸturur.
"""

# Standard Library
import logging
import os
import time
from typing import List, Optional

# Third Party
import google.generativeai as genai
import numpy as np
from dotenv import load_dotenv

# Environment variables yÃ¼kle
load_dotenv()
logger = logging.getLogger(__name__)


class EmbeddingService:
    def __init__(
        self, batch_size: int = 10, retry_count: int = 3, rate_limit_delay: float = 0.1
    ):
        """Gemini API'yi baÅŸlat ve konfigÃ¼rasyon ayarlarÄ±nÄ± sakla"""
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key or api_key == "your_gemini_api_key_here":
            raise ValueError(
                "Gemini API key geÃ§erli deÄŸil! .env dosyasÄ±nÄ± kontrol edin."
            )
        genai.configure(api_key=api_key)
        self.model = "models/text-embedding-004"
        self.batch_size = batch_size
        self.retry_count = retry_count
        self.rate_limit_delay = rate_limit_delay
        logger.info("âœ… Gemini API baÄŸlantÄ±sÄ± kuruldu")

    def create_embedding(
        self, text: str, retry_count: Optional[int] = None, max_chars: int = 8000
    ) -> Optional[List[float]]:
        """
        Tek bir metin iÃ§in embedding oluÅŸtur - Token limit kontrolÃ¼ ile
        Args:
            text: Embedding oluÅŸturulacak metin
            retry_count: Hata durumunda deneme sayÄ±sÄ± (None ise varsayÄ±lan kullanÄ±lÄ±r)
                max_chars: Maksimum karakter sayÄ±sÄ±
                (Gemini token limiti iÃ§in, ~8000 char â‰ˆ 2000 token)
        Returns:
            Embedding vektÃ¶rÃ¼ veya None
        """
        if not text:
            return None
        # Token limit iÃ§in gÃ¼venli kÄ±saltma (Gemini text-embedding-004 iÃ§in optimize)
        original_length = len(text)
        if len(text) > max_chars:
            text = text[:max_chars]
            logger.debug(
                f"Metin {original_length} karakterden {max_chars} karaktere kÄ±saltÄ±ldÄ±"
            )
        retry_count = retry_count if retry_count is not None else self.retry_count
        for attempt in range(retry_count):
            try:
                result = genai.embed_content(
                    model=self.model, content=text, task_type="retrieval_document"
                )
                return result["embedding"]  # type: ignore
            except Exception as e:
                logger.warning(
                    f"âš ï¸ Embedding hatasÄ± (deneme {attempt + 1}/"
                    f"{retry_count}): {str(e)}"
                )
                if attempt < retry_count - 1:
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    logger.error(f"âŒ Embedding oluÅŸturulamadÄ±: {text[:50]}...")
        return None

    def create_embeddings_batch(
        self,
        texts: List[str],
        batch_size: Optional[int] = None,
        retry_count: Optional[int] = None,
        rate_limit_delay: Optional[float] = None,
        max_chars: int = 8000,
    ) -> List[Optional[List[float]]]:
        """
        Birden fazla metin iÃ§in batch embedding oluÅŸtur - Token limit kontrolÃ¼ ile
        Args:
            texts: Embedding oluÅŸturulacak metinler
            batch_size: Batch boyutu (None ise varsayÄ±lan kullanÄ±lÄ±r)
            retry_count: Hata durumunda deneme sayÄ±sÄ±
            rate_limit_delay: Her istek sonrasÄ± bekleme sÃ¼resi
            max_chars: Maksimum karakter sayÄ±sÄ± (her metin iÃ§in)
        Returns:
            Embedding vektÃ¶rlerinin listesi
        """
        batch_size = batch_size if batch_size is not None else self.batch_size
        retry_count = retry_count if retry_count is not None else self.retry_count
        delay = (
            rate_limit_delay if rate_limit_delay is not None else self.rate_limit_delay
        )
        embeddings = []
        total = len(texts)
        logger.info(f"ğŸ”„ {total} metin iÃ§in embedding oluÅŸturuluyor...")
        for i in range(0, total, batch_size):
            batch = texts[i : i + batch_size]
            batch_embeddings = []
            for text in batch:
                embedding = self.create_embedding(
                    text, retry_count=retry_count, max_chars=max_chars
                )
                batch_embeddings.append(embedding)
                # Rate limiting iÃ§in kÄ±sa bekleme
                time.sleep(delay)
            embeddings.extend(batch_embeddings)
            logger.info(f"ğŸ“Š Ä°lerleme: {min(i + batch_size, total)}/{total}")
        successful_count = sum(1 for e in embeddings if e is not None)
        logger.info(f"âœ… {successful_count}/{total} embedding baÅŸarÄ±yla oluÅŸturuldu")
        return embeddings

    def calculate_similarity(
        self, text1: str, text2: str, max_chars: int = 8000
    ) -> float:
        """
        Ä°ki metin arasÄ±ndaki anlamsal benzerliÄŸi hesaplar (cosine similarity)
        Args:
            text1: Ä°lk metin
            text2: Ä°kinci metin
            max_chars: Maksimum karakter sayÄ±sÄ± (her metin iÃ§in)
        Returns:
            Benzerlik puanÄ± (0-100 arasÄ±)
        """
        logger.info("ğŸ”„ Embedding'ler oluÅŸturuluyor...")
        # Her iki metin iÃ§in embedding oluÅŸtur
        embedding1 = self.create_embedding(text1, max_chars=max_chars)
        embedding2 = self.create_embedding(text2, max_chars=max_chars)
        if embedding1 is None or embedding2 is None:
            logger.error("âŒ Embedding oluÅŸturulamadÄ±!")
            return 0.0
        # NumPy array'lere Ã§evir
        vec1 = np.array(embedding1)
        vec2 = np.array(embedding2)
        # Cosine similarity hesapla
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        cosine_sim = dot_product / (norm1 * norm2)
        # 0-100 arasÄ±na Ã¶lÃ§ekle
        similarity_percentage = (cosine_sim + 1) / 2 * 100
        logger.info(f"âœ… Cosine similarity: {cosine_sim:.4f}")
        logger.info(f"âœ… Benzerlik puanÄ±: {similarity_percentage:.2f}%")
        return float(similarity_percentage)


if __name__ == "__main__":
    # Test Ã§alÄ±ÅŸtÄ±rmasÄ±
    service = EmbeddingService()
    test_embedding = service.create_embedding("Bu bir test metnidir.")
    logger.info(
        f"Test embedding boyutu: "
        f"{len(test_embedding) if test_embedding else 'BaÅŸarÄ±sÄ±z'}"
    )
