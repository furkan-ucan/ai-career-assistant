"""
Veri Toplama ModÃ¼lÃ¼ - JobSpy GeliÅŸmiÅŸ Ã–zellikler ile Optimize EdilmiÅŸ
JobSpy'Ä±n advanced features (hours_old, geliÅŸmiÅŸ search queries, site-specific params) kullanarak
birden fazla platformdan CV'ye uygun iÅŸ ilanlarÄ±nÄ± toplar.
"""

from jobspy import scrape_jobs
import pandas as pd
from datetime import datetime
import os

# --- VARSAYILAN AYARLAR ---
VARSAYILAN_LOKASYON = "Turkey"
VARSAYILAN_MAX_SONUC_PER_SITE = 50  # Her site iÃ§in ayrÄ± limit
HEDEFLENEN_SITELER = ["indeed", "linkedin"]  # Ã‡OK Ã–NEMLÄ°: LinkedIn Ã¶ncelikli!

def collect_job_data(
    search_term,
    location=VARSAYILAN_LOKASYON,
    max_results_per_site=VARSAYILAN_MAX_SONUC_PER_SITE,
    site_names=HEDEFLENEN_SITELER,
    hours_old=72  # JobSpy native tarih filtresi (varsayÄ±lan: 3 gÃ¼n)
):
    """
    JobSpy'Ä±n geliÅŸmiÅŸ Ã¶zelliklerini kullanarak optimize edilmiÅŸ iÅŸ ilanÄ± toplama.

    Args:
        search_term: Arama terimi - Indeed iÃ§in geliÅŸmiÅŸ operatÃ¶rler desteklenir
        location: Arama lokasyonu (varsayÄ±lan: Turkey)
        max_results_per_site: Her site iÃ§in maksimum sonuÃ§ sayÄ±sÄ±
        site_names: Hedeflenen siteler listesi
        hours_old: Son X saat iÃ§indeki ilanlar (JobSpy native filtre)

    Returns:
        pandas.DataFrame: BirleÅŸtirilmiÅŸ iÅŸ ilanlarÄ± veya None (hata durumunda)
    """
    print(f"\nğŸ” JobSpy GeliÅŸmiÅŸ Arama BaÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“ Lokasyon: {location}")
    print(f"ğŸ¯ Hedef: {max_results_per_site} ilan/site")
    print(f"â° Tarih filtresi: Son {hours_old} saat (JobSpy native)")
    print(f"ğŸ” Arama terimi: '{search_term}'")
    print("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")

    all_jobs_list = []

    for site in site_names:
        print(f"\n--- Site '{site}' iÃ§in arama yapÄ±lÄ±yor ---")
        try:
            # JobSpy'Ä±n geliÅŸmiÅŸ parametreleri
            scrape_params = {
                'site_name': site,
                'search_term': search_term,
                'location': location,
                'results_wanted': max_results_per_site,
                'hours_old': hours_old  # Native tarih filtresi
            }

            # Site-specific optimizations
            if site == "indeed":
                scrape_params['country_indeed'] = "Turkey"
                print("   ğŸ¯ Indeed: TÃ¼rkiye Ã¶zel ayarlarÄ± aktif")

            elif site == "linkedin":
                scrape_params['linkedin_fetch_description'] = True  # DetaylÄ± LinkedIn verisi
                print("   ğŸ’¼ LinkedIn: DetaylÄ± aÃ§Ä±klama ve direkt URL Ã§ekiliyor...")

            # JobSpy ile veri Ã§ek
            jobs_from_site = scrape_jobs(**scrape_params)

            if jobs_from_site is not None and not jobs_from_site.empty:
                print(f"âœ… '{site}' sitesinden {len(jobs_from_site)} ilan toplandÄ±.")
                jobs_from_site['source_site'] = site  # Hangi siteden geldiÄŸini iÅŸaretle
                all_jobs_list.append(jobs_from_site)
            else:
                print(f"â„¹ï¸ '{site}' sitesinden bu arama terimi iÃ§in ilan bulunamadÄ±.")

        except Exception as e:
            print(f"âŒ '{site}' sitesinden veri toplarken hata: {str(e)}")
            continue  # Bir sitede hata olursa diÄŸerlerine devam et

    if not all_jobs_list:
        print("âŒ HiÃ§bir siteden ilan bulunamadÄ±!")
        return None

    # TÃ¼m sitelerden gelen DataFrame'leri birleÅŸtir
    combined_df = pd.concat(all_jobs_list, ignore_index=True)

    # Zaman damgasÄ± ekle
    combined_df['collected_at'] = datetime.now()

    # GeliÅŸmiÅŸ deduplication (farklÄ± sitelerden aynÄ± ilan gelebilir)
    print(f"\nğŸ”„ Deduplication baÅŸlatÄ±lÄ±yor...")
    initial_count = len(combined_df)

    if 'description' in combined_df.columns:
        # AÃ§Ä±klama varsa daha hassas deduplication
        combined_df['description_short'] = combined_df['description'].str[:100]
        combined_df.drop_duplicates(
            subset=['title', 'company', 'location', 'description_short'],
            inplace=True,
            keep='first'
        )
        combined_df.drop(columns=['description_short'], inplace=True)
    else:
        # Temel deduplication
        combined_df.drop_duplicates(
            subset=['title', 'company', 'location'],
            inplace=True,
            keep='first'
        )

    final_count = len(combined_df)
    removed_count = initial_count - final_count

    print(f"âœ¨ Deduplication tamamlandÄ±:")
    print(f"   ğŸ“Š BaÅŸlangÄ±Ã§: {initial_count} ilan")
    print(f"   ğŸ—‘ï¸ Ã‡Ä±karÄ±lan tekrar: {removed_count} ilan")
    print(f"   âœ… Final: {final_count} benzersiz ilan")

    return combined_df

# CSV kaydetme fonksiyonu (isteÄŸe baÄŸlÄ±)
def save_jobs_to_csv(jobs_df, filename_prefix="jobspy_ilanlar"):
    """
    Ä°ÅŸ ilanlarÄ±nÄ± CSV dosyasÄ±na kaydeder
    """
    if jobs_df is None or jobs_df.empty:
        print("âŒ Kaydedilecek veri yok!")
        return None

    output_dir = "data"
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = os.path.join(output_dir, f"{filename_prefix}_{timestamp}.csv")

    jobs_df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"ğŸ“ Dosya kaydedildi: {csv_path}")

    return csv_path

if __name__ == "__main__":
    # Test iÃ§in basit bir Ã§alÄ±ÅŸtÄ±rma
    print("ğŸ§ª JobSpy GeliÅŸmiÅŸ Ã–zellikler Test Ediliyor...")

    test_df = collect_job_data(
        search_term="Software Engineer",
        max_results_per_site=10,
        hours_old=72
    )

    if test_df is not None:
        print(f"\nâœ… Test sonucu: {len(test_df)} ilan bulundu.")
        print(f"ğŸ“Š Site daÄŸÄ±lÄ±mÄ±: {test_df['source_site'].value_counts().to_dict()}")
        save_jobs_to_csv(test_df, "test_advanced_jobspy")
    else:
        print("âŒ Test baÅŸarÄ±sÄ±z!")
