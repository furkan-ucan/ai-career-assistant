"""
Data collector for scraping job listings with JobSpy.

The module performs advanced searches across multiple job sites,
deduplicates the results and provides utilities to save them to CSV.
"""

# Standard Library
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

# Third Party
import pandas as pd
from jobspy import scrape_jobs

logger = logging.getLogger(__name__)

# --- VARSAYILAN AYARLAR ---
VARSAYILAN_LOKASYON = "Turkey"
VARSAYILAN_MAX_SONUC_PER_SITE = 50  # Her site iÃ§in ayrÄ± limit
HEDEFLENEN_SITELER = ["indeed", "linkedin"]  # Ã‡OK Ã–NEMLÄ°: LinkedIn Ã¶ncelikli!


def collect_job_data(
    search_term,
    location=VARSAYILAN_LOKASYON,
    max_results_per_site=VARSAYILAN_MAX_SONUC_PER_SITE,
    site_names=HEDEFLENEN_SITELER,
    hours_old=72,  # JobSpy native tarih filtresi (varsayÄ±lan: 3 gÃ¼n)
):
    """
    JobSpy'Ä±n geliÅŸmiÅŸ Ã¶zelliklerini kullanarak optimize edilmiÅŸ iÅŸ ilanÄ± toplama.

    Args:
        search_term: Arama terimi - Indeed iÃ§in geliÅŸmiÅŸ operatÃ¶rler desteklenir
        location: Arama lokasyonu (varsayÄ±lan: Turkey)
        max_results_per_site: Her site iÃ§in maksimum sonuÃ§ sayÄ±sÄ±
        site_names: Hedeflenen siteler listesi
        hours_old: Son X saat iÃ§indeki ilanlar (JobSpy native filtre)

    Returns:
        pandas.DataFrame: BirleÅŸtirilmiÅŸ iÅŸ ilanlarÄ± veya None (hata durumunda)"""
    logger.info("\nğŸ” JobSpy GeliÅŸmiÅŸ Arama BaÅŸlatÄ±lÄ±yor...")
    logger.info(f"ğŸ“ Lokasyon: {location}")
    logger.info(f"ğŸ¯ Hedef: {max_results_per_site} ilan/site")
    logger.info(f"â° Tarih filtresi: Son {hours_old} saat (JobSpy native)")
    logger.info(f"ğŸ” Arama terimi: '{search_term}'")
    logger.info("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")

    all_jobs_list = []

    def scrape_single(site: str):
        logger.info(f"\n--- Site '{site}' iÃ§in arama yapÄ±lÄ±yor ---")
        try:
            scrape_params = {
                "site_name": site,
                "search_term": search_term,
                "location": location,
                "results_wanted": max_results_per_site,
                "hours_old": hours_old,
            }
            if site == "indeed":
                scrape_params["country_indeed"] = "Turkey"
                logger.info("   ğŸ¯ Indeed: TÃ¼rkiye Ã¶zel ayarlarÄ± aktif")
            elif site == "linkedin":
                scrape_params["linkedin_fetch_description"] = True
                logger.info("   ğŸ’¼ LinkedIn: DetaylÄ± aÃ§Ä±klama ve direkt URL Ã§ekiliyor...")

            jobs_from_site = scrape_jobs(**scrape_params)
            if jobs_from_site is not None and not jobs_from_site.empty:
                logger.info(f"âœ… '{site}' sitesinden {len(jobs_from_site)} ilan toplandÄ±.")
                jobs_from_site["source_site"] = site
                return jobs_from_site
            logger.info(f"â„¹ï¸ '{site}' sitesinden bu arama terimi iÃ§in ilan bulunamadÄ±.")
        except Exception as e:
            logger.error(f"âŒ '{site}' sitesinden veri toplarken hata: {str(e)}", exc_info=True)
        return None

    with ThreadPoolExecutor(max_workers=len(site_names)) as executor:
        future_to_site = {executor.submit(scrape_single, site): site for site in site_names}
        for future in as_completed(future_to_site):
            result = future.result()
            if result is not None:
                all_jobs_list.append(result)

    if not all_jobs_list:
        logger.error("âŒ HiÃ§bir siteden ilan bulunamadÄ±!")
        return None

    # TÃ¼m sitelerden gelen DataFrame'leri birleÅŸtir
    combined_df = pd.concat(all_jobs_list, ignore_index=True)  # Zaman damgasÄ± ekle
    combined_df["collected_at"] = datetime.now()  # GeliÅŸmiÅŸ deduplication (farklÄ± sitelerden aynÄ± ilan gelebilir)
    logger.info("\nğŸ”„ Deduplication baÅŸlatÄ±lÄ±yor...")
    initial_count = len(combined_df)

    if "description" in combined_df.columns:
        # AÃ§Ä±klama varsa daha hassas deduplication
        combined_df["description_short"] = combined_df["description"].str[:100]
        combined_df.drop_duplicates(
            subset=["title", "company", "location", "description_short"],
            inplace=True,
            keep="first",
        )
        combined_df.drop(columns=["description_short"], inplace=True)
    else:
        # Temel deduplication
        combined_df.drop_duplicates(subset=["title", "company", "location"], inplace=True, keep="first")

    final_count = len(combined_df)
    removed_count = initial_count - final_count
    logger.info("âœ¨ Deduplication tamamlandÄ±:")
    logger.info(f"   ğŸ“Š BaÅŸlangÄ±Ã§: {initial_count} ilan")
    logger.info(f"   ğŸ—‘ï¸ Ã‡Ä±karÄ±lan tekrar: {removed_count} ilan")
    logger.info(f"   âœ… Final: {final_count} benzersiz ilan")

    return combined_df


# CSV kaydetme fonksiyonu (isteÄŸe baÄŸlÄ±)
def save_jobs_to_csv(jobs_df, filename_prefix="jobspy_ilanlar"):
    """
    Ä°ÅŸ ilanlarÄ±nÄ± CSV dosyasÄ±na kaydeder
    """
    if jobs_df is None or jobs_df.empty:
        logger.error("âŒ Kaydedilecek veri yok!")
        return None

    output_dir = Path("data")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = output_dir / f"{filename_prefix}_{timestamp}.csv"

    jobs_df.to_csv(csv_path, index=False, encoding="utf-8")
    logger.info(f"ğŸ“ Dosya kaydedildi: {csv_path}")

    return csv_path


if __name__ == "__main__":
    # Test iÃ§in basit bir Ã§alÄ±ÅŸtÄ±rma
    logger.info("ğŸ§ª JobSpy GeliÅŸmiÅŸ Ã–zellikler Test Ediliyor...")

    test_df = collect_job_data(search_term="Software Engineer", max_results_per_site=10, hours_old=72)

    if test_df is not None:
        logger.info(f"\nâœ… Test sonucu: {len(test_df)} ilan bulundu.")
        logger.info(f"ğŸ“Š Site daÄŸÄ±lÄ±mÄ±: {test_df['source_site'].value_counts().to_dict()}")
        save_jobs_to_csv(test_df, "test_advanced_jobspy")
    else:
        logger.error("âŒ Test baÅŸarÄ±sÄ±z!")
