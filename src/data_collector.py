"""
Veri Toplama ModÃ¼lÃ¼ - Ã‡oklu Site DesteÄŸi ile AkÄ±llÄ± Arama
JobSpy kullanarak birden fazla platformdan CV'ye uygun iÅŸ ilanlarÄ±nÄ± toplar.
"""

from jobspy import scrape_jobs
import pandas as pd
from datetime import datetime
import os

# --- VARSAYILAN AYARLAR ---
VARSAYILAN_LOKASYON = "Turkey"
VARSAYILAN_MAX_SONUC_PER_SITE = 50  # Her site iÃ§in ayrÄ± limit
HEDEFLENEN_SITELER = ["indeed", "linkedin"]  # Ã‡OK Ã–NEMLÄ°: Ã–nce Indeed (daha stabil), sonra LinkedIn

def collect_job_data(
    search_term,
    location=VARSAYILAN_LOKASYON,
    max_results_per_site=VARSAYILAN_MAX_SONUC_PER_SITE,
    site_names=HEDEFLENEN_SITELER
):
    """
    Belirtilen sitelerden ve parametrelerle iÅŸ ilanlarÄ±nÄ± toplar.

    Args:
        search_term: Arama terimi (zorunlu)
        location: Arama lokasyonu (varsayÄ±lan: Turkey)
        max_results_per_site: Her site iÃ§in maksimum sonuÃ§ sayÄ±sÄ±
        site_names: Hedeflenen siteler listesi

    Returns:
        pandas.DataFrame: BirleÅŸtirilmiÅŸ iÅŸ ilanlarÄ± veya None (hata durumunda)
    """
    print(f"\nğŸ” AkÄ±llÄ± Ä°ÅŸ Arama BaÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“ Lokasyon: {location}")
    print(f"ğŸ¯ Hedef: {max_results_per_site} ilan")
    print("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir (geniÅŸ spektrum arama)...")

    print("ğŸ¯ AkÄ±llÄ± arama spektrumu:")
    print("   - Full Stack, React, NestJS, TypeScript")
    print("   - Veri Analisti, Ä°ÅŸ Analisti, ERP")
    print("   - Flutter, Python, GIS, Data Science")

    all_jobs_list = []

    for site in site_names:
        print(f"\n--- Site '{site}' iÃ§in arama yapÄ±lÄ±yor ---")
        try:
            # Site-specific parameters
            scrape_params = {
                'site_name': site,
                'search_term': search_term,
                'location': location,
                'results_wanted': max_results_per_site,
                'hours_old': 72  # Son 3 gÃ¼n iÃ§in (JobSpy native date filter)
            }

            # Site-specific optimizations
            if site == "indeed":
                scrape_params['country_indeed'] = "Turkey"
            elif site == "linkedin":
                scrape_params['linkedin_fetch_description'] = True  # Daha detaylÄ± LinkedIn verisi
                print("   ğŸ’¼ LinkedIn: DetaylÄ± aÃ§Ä±klama ve direkt URL Ã§ekiliyor...")

            jobs_from_site = scrape_jobs(**scrape_params)

            if jobs_from_site is not None and not jobs_from_site.empty:
                print(f"âœ… '{site}' sitesinden {len(jobs_from_site)} ilan toplandÄ±.")
                jobs_from_site['source_site'] = site  # Hangi siteden geldiÄŸini iÅŸaretle
                all_jobs_list.append(jobs_from_site)
            else:
                print(f"â„¹ï¸ '{site}' sitesinden bu arama terimi iÃ§in ilan bulunamadÄ±.")

        except Exception as e:
            print(f"âŒ '{site}' sitesinden veri toplarken hata: {str(e)}")
            continue  # Bir sitede hata olursa diÄŸerlerine devam et

    if not all_jobs_list:
        print("âŒ HiÃ§bir siteden ilan bulunamadÄ±!")
        return None

    # TÃ¼m sitelerden gelen DataFrame'leri birleÅŸtir
    combined_df = pd.concat(all_jobs_list, ignore_index=True)

    # Zaman damgasÄ± ekle
    combined_df['collected_at'] = datetime.now()

    # Tekrarlanan ilanlarÄ± temizle (farklÄ± sitelerden aynÄ± ilan gelebilir)
    if 'description' in combined_df.columns:
        combined_df['description_short'] = combined_df['description'].str[:100]
        combined_df.drop_duplicates(subset=['title', 'company', 'location', 'description_short'], inplace=True, keep='first')
        combined_df.drop(columns=['description_short'], inplace=True)
    else:
        combined_df.drop_duplicates(subset=['title', 'company', 'location'], inplace=True, keep='first')

    print(f"\nâœ¨ Toplamda {len(combined_df)} adet benzersiz ilan (tÃ¼m sitelerden) bulundu.")

    return combined_df  # DataFrame dÃ¶ndÃ¼r

# CSV kaydetme fonksiyonu (isteÄŸe baÄŸlÄ±)
def save_jobs_to_csv(jobs_df, filename_prefix="ham_ilanlar"):
    """
    Ä°ÅŸ ilanlarÄ±nÄ± CSV dosyasÄ±na kaydeder
    """
    if jobs_df is None or jobs_df.empty:
        print("âŒ Kaydedilecek veri yok!")
        return None

    output_dir = "data"
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = os.path.join(output_dir, f"{filename_prefix}_{timestamp}.csv")

    jobs_df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"ğŸ“ Dosya kaydedildi: {csv_path}")

    return csv_path

# Eski main.py uyumluluÄŸu iÃ§in wrapper fonksiyon
def collect_job_data_legacy(search_term, location=VARSAYILAN_LOKASYON, max_results=20):
    """
    Eski main.py uyumluluÄŸu iÃ§in wrapper. DataFrame yerine CSV path dÃ¶ndÃ¼rÃ¼r.
    """
    jobs_df = collect_job_data(
        search_term=search_term,
        location=location,
        max_results_per_site=max_results,
        site_names=["indeed"]  # Eski davranÄ±ÅŸ iÃ§in sadece Indeed
    )

    if jobs_df is not None:
        return save_jobs_to_csv(jobs_df)
    else:
        return None

if __name__ == "__main__":
    # Test iÃ§in basit bir Ã§alÄ±ÅŸtÄ±rma
    test_df = collect_job_data(search_term="Python Developer", max_results_per_site=10)
    if test_df is not None:
        print(f"\nTest sonucu: {len(test_df)} ilan bulundu.")
        print(f"Siteler: {test_df['source_site'].value_counts().to_dict()}")
        save_jobs_to_csv(test_df, "test_multi_site")
